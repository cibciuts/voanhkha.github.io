---
layout: post
title: The Magic of Reinforcement Learning (part 1)
---

In this post, I will present some interesting and somewhat "magical" (at least, to me :-) ) stuffs about Reinforcement Learning (RL), via some simple scenarios.
By the end of this post, I hope you will also be thrilled by the capability of what RL can bring.

This is the first part of a planned 3 parts I want to write about RL. These posts are for everybody even one with not much prior knowledge in RL, to understand the great power of the stochasticity property of the behaviour of the learned agents.

![_config.yml](/images/MAgentGIF.gif)

![_config.yml](/images/magic_rl_p1_image1.png)

In this first scenario demonstration, I put 1 agent and 2 foods on random positions of the map. Since the agent can move in 4 directions at any time step, they can theoretically reach any cell in this 6x6 map, or any cell in a much much bigger map, say, 1 billion x 1 billion. That means they can theoretically collect as many foods as possible, on an extremely big map.

Turning back on this tiny 6x6 world, I introduce the term "state", or "observation" of the agent, that is the 3x3 square with the agent in the center. That is equivalent to saying that at any time step, the agent knows (and only knows) the nearest 9 cells surrounding it. Numerically, I can represent each blank cell (no food) as 0, the food-occupied cell as 1, and the wall cells as -1. Indeed, each cell type can be represented by any arbitrary number, because no matter what our model can still automatically adjust this change during training. But it's better to keep things as simple as possible for the sake of our model training task later on. By the way, these environment objects' representation makes sense, since food cell (1) is more encouraged to reach, blank cell (0) is uncertain, and wall cell (-1) is encouraged to avoid (by the environment's definition the agent will stay as-is if it continues to collide with the wall).
